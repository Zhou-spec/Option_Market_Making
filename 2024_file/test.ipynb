{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from simulation import *\n",
    "from policy_simulation import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2003], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a neural network for the policy \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2 + 2 * n, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 1)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # add relu to make sure the output is positive\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "value_net = Net(2)\n",
    "\n",
    "value_net.forward(torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have the a path of trading, we can compute the loss function \n",
    "\n",
    "def temporal_diff_error(new_value_net, policy, t, Q, P, S, t_prime, Q_prime, P_prime, S_prime, dt, delta, gamma):\n",
    "    # new_value_net is the value network to estimate the value function of the current policy\n",
    "    # policy is the trading policy object\n",
    "    # (t, Q, P, S) is the current state\n",
    "    # (t', Q', P', S') is the next state\n",
    "    # dt is the time step\n",
    "    # delta is the delta of the option\n",
    "    # gamma is the gamma of the option\n",
    "\n",
    "    # the difference between the value function\n",
    "    # concatenate (t, Q, P, S) and (t', Q', P', S')\n",
    "    state = np.concatenate([t, Q, P, S])\n",
    "    state_prime = np.concatenate([t_prime, Q_prime, P_prime, S_prime])\n",
    "\n",
    "    # transfer the state to tensor\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    state_prime = torch.tensor(state_prime, dtype=torch.float32)\n",
    "\n",
    "    value_diff = new_value_net.forward(state_prime) - new_value_net.forward(state)\n",
    "    value_diff /= dt\n",
    "\n",
    "    # compute the expected reward \n",
    "    profits = policy.expected_profits(t, Q, P, S)\n",
    "\n",
    "    # compute the option related penalty\n",
    "    option_penalty = np.dot(Q, (delta + gamma))\n",
    "\n",
    "    # compute the entropy \n",
    "    entropy = policy.policy_entropy(t, Q, P, S)\n",
    "\n",
    "\n",
    "    error = profits + value_diff + option_penalty + entropy\n",
    "\n",
    "    return (error**2) * dt\n",
    "\n",
    "\n",
    "# this is to compute the loss function of one simulated trajectories\n",
    "# new_value_net is to estimate the value function of the current policy\n",
    "# new_value_net is the only network to be trained    \n",
    "def martingale_loss(new_value_net, policy, stock_price_path, options_price_path, options_delta_path, options_gamma_path, inv_path, dt, T):\n",
    "    # new_value_net is the value network to estimate the value function of the current policy\n",
    "    # policy is the trading policy object\n",
    "    # stock_price_path is the path of stock price\n",
    "    # options_price_path is the path of option price\n",
    "    # options_delta_path is the path of option delta\n",
    "    # options_gamma_path is the path of option gamma\n",
    "    # inv_path is the path of inventory\n",
    "    # dt is the time step\n",
    "    # T is the maturity of the option\n",
    "\n",
    "    N = int(T / dt)\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(N - 1):\n",
    "        t = np.array([i * dt])\n",
    "        t_prime = np.array([(i + 1) * dt])\n",
    "\n",
    "        Q = inv_path[i]\n",
    "        Q_prime = inv_path[i + 1]\n",
    "\n",
    "        P = options_price_path[i]\n",
    "        P_prime = options_price_path[i + 1]\n",
    "\n",
    "        S = np.array([stock_price_path[i]])\n",
    "        S_prime = np.array([stock_price_path[i + 1]])\n",
    "\n",
    "        delta = options_delta_path[i]\n",
    "        gamma = options_gamma_path[i]\n",
    "\n",
    "        loss += temporal_diff_error(new_value_net, policy, t, Q, P, S, t_prime, Q_prime, P_prime, S_prime, dt, delta, gamma)\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let bid_range be n*2 array\n",
    "bid_range = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9]])\n",
    "ask_range = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9]])\n",
    "\n",
    "t = np.array([0])\n",
    "dt = 0.01\n",
    "Q = np.array([0, 0])\n",
    "P = np.array([12, 23])\n",
    "S = np.array([1])   \n",
    "penalty = 10\n",
    "A = 30\n",
    "kappa = 3\n",
    "#print(policy_distribution(value_net, t, Q, P, S, penalty, A, kappa, bid_range, ask_range))\n",
    "\n",
    "\n",
    "# define trading policy object\n",
    "policy = TradingPolicy(value_net, penalty, A, kappa, bid_range, ask_range)\n",
    "# define a new value network\n",
    "new_value_net = Net(2)\n",
    "\n",
    "\n",
    "# generate the training data \n",
    "V = np.array([[1, 0.5], [0.5, 1]])\n",
    "stock_path = stock_price_path(100, 0.05, 1, 0.01)\n",
    "K = np.array([30, 50])\n",
    "daily_sigma = 0.01\n",
    "option_price_path, delta_path, gamma_path = option_simulation(V, stock_path, 1, 0.01, K, 0.01, daily_sigma) \n",
    "\n",
    "inv, buy, sell = entire_trading(policy, option_price_path, stock_path, 0.01, A, kappa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss function \n",
    "loss = martingale_loss(new_value_net, policy, stock_path, option_price_path, delta_path, gamma_path, inv, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 0\n",
      "loss: tensor([118.6872], grad_fn=<AddBackward0>)\n",
      "training epoch: 1\n",
      "loss: tensor([60.6772], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print('training epoch:', i)\n",
    "    option_price_path, delta_path, gamma_path = option_simulation(V, stock_path, 1, 0.01, K, 0.01, daily_sigma)\n",
    "    inv, buy, sell = entire_trading(policy, option_price_path, stock_path, 0.01, A, kappa)\n",
    "\n",
    "    loss = martingale_loss(new_value_net, policy, stock_path, option_price_path, delta_path, gamma_path, inv, 0.01, 1)\n",
    "    print('loss:', loss)\n",
    "\n",
    "    # update the value network\n",
    "    loss.backward()\n",
    "\n",
    "    # update the value network\n",
    "    optimizer = torch.optim.Adam(new_value_net.parameters(), lr=0.01)\n",
    "    optimizer.step()\n",
    "\n",
    "    # clear the gradient\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qids-2023-comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
